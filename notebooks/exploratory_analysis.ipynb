{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Cleaning and Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/online_retail.csv')\n",
    "print(f'Rows, Columns count', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are present in `Description, CustomerID`, `InvoiceDate, CustomerID` have incorrect datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our numerical columns are highly skewed, they also appear to have invalid values; there are negative values present\n",
    "\n",
    "**2. Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# Check for and drop missing values for critical columns\n",
    "print(f'Missing values per column\\n',data.isnull().sum())\n",
    "data.dropna(subset=['InvoiceDate', 'CustomerID', 'InvoiceDate'], inplace=True)\n",
    "\n",
    "# Change invoiceDate to DateTime format\n",
    "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'], errors='coerce')\n",
    "# Convert customerID to str\n",
    "data['CustomerID'] = data['CustomerID'].astype(int).astype(str)\n",
    "\n",
    "# Filter invalid values in Quantity and UnitPrice columns\n",
    "data = data[(data['Quantity'] > 0) & (data['UnitPrice'] > 0.01)]\n",
    "\n",
    "# Create TransactionAmount columns\n",
    "data['TransactionAmount'] = (data['UnitPrice'] * data['Quantity']).round(2)\n",
    "\n",
    "# Inspection check\n",
    "print(f'Missing values after cleaning:\\n', data.isna().sum(), '\\n')\n",
    "print(f'Updated Rows, Columns:', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking Duplicate Entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicate Entries: ', data.duplicated().sum()) # 5192 Duplicates\n",
    "duplicates = data[data.duplicated(keep=False)]\n",
    "duplicates.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_groups = data[data.duplicated(keep=False)].groupby(list(data.columns)).size()\n",
    "print(len(duplicate_groups))\n",
    "duplicate_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates but keep 1st occurence of a duplicated entry\n",
    "data = data.drop_duplicates(keep='last')\n",
    "print(f'Duplicated Entries: ', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[['Quantity', 'TransactionAmount']].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = series.quantile(lower_quantile)\n",
    "    upper_bound = series.quantile(upper_quantile)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# # Automatically apply capping to all numeric columns\n",
    "# data[data.select_dtypes(include=['float64', 'int64']).columns] = data.select_dtypes(include=['float64', 'int64']).apply(\n",
    "#     lambda x: cap_outliers(x)\n",
    "# )\n",
    "# Columns to cap outliers\n",
    "columns_to_cap = ['Quantity', 'TransactionAmount']  # Replace with your column names\n",
    "\n",
    "# Apply the capping function to each column\n",
    "for col in columns_to_cap:\n",
    "    data[col] = cap_outliers(data[col])\n",
    "\n",
    "# Check summary statistics to confirm\n",
    "data.describe(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Skewness before Log-Transform after removing Outliers: \\n', data[['Quantity', 'TransactionAmount']].skew(),'\\n')\n",
    "# print(f'Skewness after Log-Transform: \\n', data[['Quantity_log', 'TransactionAmount_log']].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates check\n",
    "print(f'Number of Duplicates:', data.duplicated().sum(), '\\n')\n",
    "# Drop duplicates\n",
    "data.drop_duplicates(keep='first', inplace=True)\n",
    "# Inpect duplicate entries (Should return zero entries)\n",
    "data[data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "data.to_pickle('../data/cleaned_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/cleaned_data.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculate RFM Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Reference data\n",
    "reference_date = df['InvoiceDate'].max() + pd.DateOffset(1)\n",
    "reference_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM Metrics\n",
    "rfm = df.groupby('CustomerID').agg(\n",
    "    Recency=('InvoiceDate', lambda x: (reference_date - x.max()).days),\n",
    "    Frequency=('InvoiceNo', 'nunique'),\n",
    "    Monetary=('TransactionAmount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# RFM table preview\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness of RFM metrics\n",
    "print(rfm[[\"Recency\", \"Frequency\", \"Monetary\"]].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-Transform RFM Metrics\n",
    "rfm[[\"Recency_log\", \"Frequency_log\", \"Monetary_log\"]] = rfm[[\"Recency\", \"Frequency\", \"Monetary\"]].apply(lambda x: np.log1p(x)).round(2)\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm[[\"Recency_log\", \"Frequency_log\", \"Monetary_log\"]].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RFM Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm['Recency'] = pd.qcut(rfm['Recency'], 5, labels=[5,4,3,2,1])\n",
    "rfm['Frequency'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "rfm['Monetary'] = pd.qcut(rfm['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "\n",
    "# Combine scores into one\n",
    "rfm['RFM_score'] = rfm['Recency'].astype(int) + rfm['Frequency'].astype(int) + rfm['Monetary'].astype(int)\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Value Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_labels = ['Low-value', 'Mid-value', 'High-value']\n",
    "rfm['Value_segment'] = pd.qcut(rfm['RFM_score'], q=3, labels=value_labels)\n",
    "# Preview\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Behavioral Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behaviour_segment(row):\n",
    "    if row['RFM_score'] >= 13:\n",
    "        return 'VIP'\n",
    "    if row['RFM_score'] >= 10:\n",
    "        return 'Loyal Customers'\n",
    "    if row['RFM_score'] >= 7:\n",
    "        return 'Potential Loyalists'\n",
    "    if row['RFM_score'] >= 5:\n",
    "        return 'At Risk'\n",
    "    else:\n",
    "        return 'Hibernating'\n",
    "    \n",
    "# Applying the segmentation strategy\n",
    "rfm['Customer_segment'] = rfm.apply(behaviour_segment, axis=1)\n",
    "print(rfm['Customer_segment'].value_counts())\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the RFM metrics back to the original dataframe\n",
    "df_combined = df.merge(rfm, on='CustomerID', how='left')\n",
    "print(df_combined.shape)\n",
    "df_combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined DataFrame\n",
    "df_combined.to_pickle('../data/combined_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview of Segments (RFM Analysis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined DataFrame\n",
    "rfm_df = pd.read_pickle('../data/combined_data.pkl')\n",
    "rfm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_df[['Recency', 'Frequency', 'Monetary']] = rfm_df[['Recency', 'Frequency', 'Monetary']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the segments distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value segments\n",
    "value_counts = rfm_df['Value_segment'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "# Plot distribution\n",
    "sns.countplot(data=rfm_df, x='Value_segment', order=value_counts.index, palette=\"inferno\")\n",
    "plt.title('Distribution of Value Segments')\n",
    "plt.xlabel('Value Segment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Investigating compositions of Value segments by customer segments**.\n",
    "\n",
    "Group by Value_segment and calculate the percentage of Customer_segment within each value segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition = pd.crosstab(\n",
    "    rfm['Value_segment'], \n",
    "    rfm['Customer_segment'], \n",
    "    normalize='index'\n",
    ").mul(100).reset_index()\n",
    "# print(composition.head())\n",
    "\n",
    "pivot_data = composition.set_index('Value_segment')\n",
    "pivot_data.plot(kind='bar', stacked=True, colormap='inferno', figsize=(10, 6))\n",
    "plt.title('Composition of Value Segments by Customer Segments')\n",
    "plt.xlabel('Value Segment')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend(title='Customer Segment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze Revenue contribution by value segment**\n",
    "\n",
    "*Objective: Understand financial value of each segment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_by_value_seg = rfm_df.groupby('Value_segment')['TransactionAmount'].sum()\n",
    "print(revenue_by_value_seg)\n",
    "\n",
    "# Plot a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    revenue_by_value_seg, \n",
    "    labels=revenue_by_value_seg.index,  # Add segment labels\n",
    "    autopct='%1.1f%%',                 # Show percentage\n",
    "    startangle=320,                    # Rotate for better view\n",
    "    textprops={'fontsize': 14}         # Adjust text size\n",
    ")\n",
    "plt.legend(\n",
    "    wedges, revenue_by_value_seg.index,\n",
    "    title=\"Value Segments\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5)\n",
    ")\n",
    "plt.title('Revenue Contribution by Value Segment', fontsize=14)\n",
    "plt.tight_layout()  # Adjust layout to fit everything\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Engagement Analysis within Value Segments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_by_value_seg = rfm_df.groupby('Value_segment')['Frequency_log'].mean()\n",
    "engagement_by_value_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for Seaborn\n",
    "engagement_df = engagement_by_value_seg.reset_index()\n",
    "engagement_df.columns = ['Value Segment', 'Average Engagement']\n",
    "\n",
    "# Plot a heatmap\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(\n",
    "    engagement_df.pivot_table(index='Value Segment', values='Average Engagement'), \n",
    "    annot=True, fmt=\".2f\", cmap='Blues', cbar_kws={'label': 'Average Engagement'}\n",
    ")\n",
    "plt.title('Customer Engagement Heatmap by Value Segment', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trend Analysis of Value Segment Behavior**\n",
    "\n",
    "*Objective: Understand how the purchasing behavior of each value segment changes over time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_df['InvoiceDate'] = pd.to_datetime(rfm_df['InvoiceDate'])\n",
    "rfm_df.set_index('InvoiceDate', inplace=True)\n",
    "trend_by_segment = rfm_df.groupby(['Value_segment']).resample('M')['TransactionAmount'].sum().unstack()\n",
    "# trend_by_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for seaborn\n",
    "trend_df = trend_by_segment.stack().reset_index()\n",
    "trend_df.columns = ['Value Segment', 'Month', 'Transaction Amount']\n",
    "\n",
    "# Plot line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=trend_df,\n",
    "    x='Month', y='Transaction Amount', hue='Value Segment', marker='o'\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Monthly Revenue Trend by Value Segment', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Revenue (Transaction Amount)', fontsize=12)\n",
    "\n",
    "# Improve date readability\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Churn Risk and Retention Analysis**\n",
    "\n",
    "*Objective: Evaluate the churn risk for each value segment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_by_value_segment = rfm_df.groupby('Value_segment')['Recency_log'].mean()  # Longer recency = higher churn risk\n",
    "churn_by_value_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a dot plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(churn_by_value_segment.index, churn_by_value_segment, color=['red', 'orange', 'green'], s=100)\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Average Recency (Log Scale)', fontsize=12)\n",
    "plt.xlabel('Value Segment', fontsize=12)\n",
    "plt.title('Churn Risk by Value Segment', fontsize=14)\n",
    "\n",
    "# Add values near the dots\n",
    "for index, value in enumerate(churn_by_value_segment):\n",
    "    plt.text(index, value + 0.1, f'{value:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "# Display the chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Sell and Upsell Potential by Value Segment**\n",
    "\n",
    "*Objective*: Identify potential for cross-selling and upselling within value segments.\n",
    "\n",
    "*Approach*:\n",
    "For each value segment, examine which products are frequently purchased together or which higher-value products can be suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sell_potential = rfm_df.groupby(['Value_segment', 'StockCode'])['TransactionAmount'].sum().unstack()\n",
    "cross_sell_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate top 10 products by value segment\n",
    "top_products_by_segment = cross_sell_potential.sum(axis=1).nlargest(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for segment in cross_sell_potential.index:\n",
    "    top_products = cross_sell_potential.loc[segment].nlargest(10)  # Top 10 products for the segment\n",
    "    plt.barh(top_products.index, top_products.values, label=segment)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Transaction Amount', fontsize=12)\n",
    "plt.ylabel('Product (StockCode)', fontsize=12)\n",
    "plt.title('Top 10 Products by Value Segment', fontsize=14)\n",
    "plt.legend(title=\"Value Segment\", loc='upper right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geographic and Demographic Insights by Value Segment**\n",
    "\n",
    "*Objective:* \n",
    "\n",
    "Examine how location (Country) and demographic factors (e.g., age, gender, etc.) influence the purchasing behavior of each value segment.\n",
    "\n",
    "*Approach:*\n",
    "\n",
    "Look at the geographical distribution of each segment.\n",
    "For example, VIP customers may be more concentrated in certain regions or countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geographic_by_segment = rfm_df.groupby(['Country', 'Value_segment'])['TransactionAmount'].sum().unstack()\n",
    "geographic_by_segment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 10 countries\n",
    "top_countries = geographic_by_segment.sum(axis=1).nlargest(10).index\n",
    "stacked_data = geographic_by_segment.loc[top_countries]\n",
    "\n",
    "# Plot a stacked bar chart\n",
    "stacked_data.plot(kind='barh', stacked=True, figsize=(12, 8), colormap='viridis')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Total Revenue', fontsize=12)\n",
    "plt.ylabel('Country', fontsize=12)\n",
    "plt.title('Top 10 Countries by Revenue and Value Segment', fontsize=14)\n",
    "plt.legend(title='Value Segment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Marketing Effectiveness by Segment**\n",
    "\n",
    "*Objective:* \n",
    "\n",
    "Evaluate how effective different marketing campaigns are for each segment.\n",
    "\n",
    "*Approach*:\n",
    "\n",
    "Compare conversion rates, response to promotions, and discount redemption rates across value segments.\n",
    "Flow to Next: Allows the retailer to refine their marketing strategies for each value segment based on the campaign's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavioral Change and Predictive Modeling**\n",
    "\n",
    "*Objective:* \n",
    "\n",
    "Predict how customers will move between value segments or within the same segment over time.\n",
    "Use predictive models to understand which customers are likely to move between segments (e.g., from Potential Loyalists to Loyal Customers). For example:\n",
    "\n",
    "- Predict if a Potential Loyalist is likely to become a Loyal Customer.\n",
    "- Predict if an At Risk customer will churn or make a repeat purchase.\n",
    "\n",
    "*Approach*:\n",
    "\n",
    "Use machine learning models like logistic regression or decision trees to predict segment transitions based on recency, frequency, and monetary data. \n",
    "\n",
    "We'll start by setting the church threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define churn threshold\n",
    "churn_threshold = rfm_df['Recency_log'].quantile(0.75) # This sets the churn threshold at 75th percentile of Recency meaning customers whose Recency exceeds\n",
    "                                                       # this threshold are considered at risk of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi-class transitions\n",
    "# def label_transition(row):\n",
    "#     if row['Customer_segment'] == 'At Risk' and row['Recency_log'] > churn_threshold:\n",
    "#         return 'Churned'\n",
    "#     elif row['Customer_segment'] == 'Potential Loyalist' and row['Value_segment'] == 'High Value':\n",
    "#         return 'Upgraded'\n",
    "#     elif row['Customer_segment'] == 'VIP' and row['Value_segment'] == 'Low Value':\n",
    "#         return 'Downgraded'\n",
    "#     else:\n",
    "#         return 'No Change'\n",
    "\n",
    "# rfm['Transition'] = rfm.apply(label_transition, axis=1)\n",
    "\n",
    "\n",
    "# Binary classification: Customer who haven't purchased in a set period\n",
    "rfm_df['Churned'] = (rfm_df['Recency_log'] > churn_threshold).astype(int) # 1 is at risk of churning, 0 is active\n",
    "# Validate column \n",
    "rfm_df['Churned'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Final Recommendations for Action**\n",
    "\n",
    "*Objective:* \n",
    "\n",
    "Provide actionable insights based on the combined analysis.\n",
    "\n",
    "*Approach:*\n",
    "\n",
    "Use the findings to create a segment-based marketing strategy, targeting each value segment with appropriate strategies like loyalty rewards, re-engagement campaigns, or personalized offers.\n",
    "Flow to Next: This can result in ongoing tracking of performance, where you'll reanalyze your value segments periodically to refine strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
