{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Cleaning and Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/online_retail.csv')\n",
    "print(f'Rows, Columns count', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are present in `Description, CustomerID`, `InvoiceDate, CustomerID` have incorrect datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our numerical columns are highly skewed, they also appear to have invalid values; there are negative values present\n",
    "\n",
    "**2. Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# Check for and drop missing values for critical columns\n",
    "print(f'Missing values per column\\n',data.isnull().sum())\n",
    "data.dropna(subset=['InvoiceDate', 'CustomerID', 'InvoiceDate'], inplace=True)\n",
    "\n",
    "# Change invoiceDate to DateTime format\n",
    "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'], errors='coerce')\n",
    "# Convert customerID to str\n",
    "data['CustomerID'] = data['CustomerID'].astype(int).astype(str)\n",
    "\n",
    "# Filter invalid values in Quantity and UnitPrice columns\n",
    "data = data[(data['Quantity'] > 0) & (data['UnitPrice'] > 0.01)]\n",
    "\n",
    "# Create TransactionAmount columns\n",
    "data['TransactionAmount'] = (data['UnitPrice'] * data['Quantity']).round(2)\n",
    "\n",
    "# Inspection check\n",
    "print(f'Missing values after cleaning:\\n', data.isna().sum(), '\\n')\n",
    "print(f'Updated Rows, Columns:', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking Duplicate Entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicate Entries: ', data.duplicated().sum()) # 5192 Duplicates\n",
    "duplicates = data[data.duplicated(keep=False)]\n",
    "duplicates.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_groups = data[data.duplicated(keep=False)].groupby(list(data.columns)).size()\n",
    "print(len(duplicate_groups))\n",
    "duplicate_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates but keep 1st occurence of a duplicated entry\n",
    "data = data.drop_duplicates(keep='last')\n",
    "print(f'Duplicated Entries: ', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[['Quantity', 'TransactionAmount']].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = series.quantile(lower_quantile)\n",
    "    upper_bound = series.quantile(upper_quantile)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# # Automatically apply capping to all numeric columns\n",
    "# data[data.select_dtypes(include=['float64', 'int64']).columns] = data.select_dtypes(include=['float64', 'int64']).apply(\n",
    "#     lambda x: cap_outliers(x)\n",
    "# )\n",
    "# Columns to cap outliers\n",
    "columns_to_cap = ['Quantity', 'TransactionAmount']  # Replace with your column names\n",
    "\n",
    "# Apply the capping function to each column\n",
    "for col in columns_to_cap:\n",
    "    data[col] = cap_outliers(data[col])\n",
    "\n",
    "# Check summary statistics to confirm\n",
    "data.describe(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Skewness before Log-Transform after removing Outliers: \\n', data[['Quantity', 'TransactionAmount']].skew(),'\\n')\n",
    "# print(f'Skewness after Log-Transform: \\n', data[['Quantity_log', 'TransactionAmount_log']].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates check\n",
    "print(f'Number of Duplicates:', data.duplicated().sum(), '\\n')\n",
    "# Drop duplicates\n",
    "data.drop_duplicates(keep='first', inplace=True)\n",
    "# Inpect duplicate entries (Should return zero entries)\n",
    "data[data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "data.to_pickle('../data/cleaned_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/cleaned_data.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculate RFM Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Reference data\n",
    "reference_date = df['InvoiceDate'].max() + pd.DateOffset(1)\n",
    "reference_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM Metrics\n",
    "rfm = df.groupby('CustomerID').agg(\n",
    "    Recency=('InvoiceDate', lambda x: (reference_date - x.max()).days),\n",
    "    Frequency=('InvoiceNo', 'nunique'),\n",
    "    Monetary=('TransactionAmount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# RFM table preview\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness of RFM metrics\n",
    "print(rfm[[\"Recency\", \"Frequency\", \"Monetary\"]].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-Transform RFM Metrics\n",
    "rfm[[\"Recency_log\", \"Frequency_log\", \"Monetary_log\"]] = rfm[[\"Recency\", \"Frequency\", \"Monetary\"]].apply(lambda x: np.log1p(x)).round(2)\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm[[\"Recency_log\", \"Frequency_log\", \"Monetary_log\"]].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RFM Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm['Recency_log'] = pd.qcut(rfm['Recency_log'], 5, labels=[5,4,3,2,1])\n",
    "rfm['Frequency_log'] = pd.qcut(rfm['Frequency_log'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "rfm['Monetary_log'] = pd.qcut(rfm['Monetary_log'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "\n",
    "# Combine scores into one\n",
    "rfm['RFM_score'] = rfm['Recency_log'].astype(int) + rfm['Frequency_log'].astype(int) + rfm['Monetary_log'].astype(int)\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Value Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_labels = ['Low-value', 'Mid-value', 'High-value']\n",
    "rfm['Value_segment'] = pd.qcut(rfm['RFM_score'], q=3, labels=value_labels)\n",
    "# Preview\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Behavioral Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behaviour_segment(row):\n",
    "    if row['RFM_score'] >= 13:\n",
    "        return 'VIP'\n",
    "    if row['RFM_score'] >= 10:\n",
    "        return 'Loyal Customers'\n",
    "    if row['RFM_score'] >= 7:\n",
    "        return 'Potential Loyalists'\n",
    "    if row['RFM_score'] >= 5:\n",
    "        return 'At Risk'\n",
    "    else:\n",
    "        return 'Hibernating'\n",
    "    \n",
    "# Applying the segmentation strategy\n",
    "rfm['Customer_segment'] = rfm.apply(behaviour_segment, axis=1)\n",
    "print(rfm['Customer_segment'].value_counts())\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the RFM metrics back to the original dataframe\n",
    "df_combined = df.merge(rfm, on='CustomerID', how='left')\n",
    "print(df_combined.shape)\n",
    "df_combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined DataFrame\n",
    "df_combined.to_pickle('../data/combined_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview of Segments (RFM Analysis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined DataFrame\n",
    "rfm_df = pd.read_pickle('../data/combined_data.pkl')\n",
    "rfm_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the segments distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value segments\n",
    "value_counts = rfm_df['Value_segment'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "# Plot distribution\n",
    "sns.countplot(data=rfm_df, x='Value_segment', order=value_counts.index, palette=\"viridis\")\n",
    "plt.title('Distribution of Value Segments')\n",
    "plt.xlabel('Value Segment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
